{"cells":[{"cell_type":"markdown","metadata":{"id":"oGMX97NcEbWI"},"source":["## Word Embeddings in Python with Gensim"]},{"cell_type":"markdown","metadata":{"id":"a_YIkKhxEZ5x"},"source":["https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"]},{"cell_type":"markdown","metadata":{"id":"ldkfQlDaHr8D"},"source":["## Word2vec by Google and GloVe by Stanford"]},{"cell_type":"markdown","metadata":{"id":"ihnc0sVvu9H1"},"source":["Word embeddings are an **improvement over simpler bag-of-words model** word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words. It provides a **dense vector representation of words that capture something about their meaning**.\n","\n","It is **defining a word by the company that it keeps** that allows the word embedding to learn something about the meaning of words. The vector space representation of the words provides a projection where words with similar meanings are locally clustered within the space.\n","\n","We are going to look at how to use two different word embedding methods called **word2vec by researchers at Google and GloVe by researchers at Stanford**."]},{"cell_type":"markdown","metadata":{"id":"33sO1pYiIEAy"},"source":["## Gensim library"]},{"cell_type":"markdown","metadata":{"id":"RT7i5Snfu9IE"},"source":["Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling\n","1. It supports an implementation of the Word2Vec word embedding for **learning new word vectors** from text\n","2. It also provides tools for **loading pre-trained word embeddings** in a few formats and for making use and querying a loaded embedding."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"je5lFkeyvBRl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685144915029,"user_tz":-330,"elapsed":25597,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}},"outputId":"e2754aab-8ce6-4d89-ce8e-81b5853c6edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"]}],"source":["!pip install nltk\n","!pip install gensim"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZJUQOg-fu9H5","executionInfo":{"status":"ok","timestamp":1685144920201,"user_tz":-330,"elapsed":2115,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}}},"outputs":[],"source":["from gensim.models import Word2Vec"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1qCeQ8xdu9IG","executionInfo":{"status":"ok","timestamp":1685144921493,"user_tz":-330,"elapsed":160,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}}},"outputs":[],"source":["# define training data\n","sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n","            ['this', 'is', 'the', 'second', 'sentence'],\n","            ['yet', 'another', 'sentence'],\n","            ['one', 'more', 'sentence'],\n","            ['and', 'the', 'final', 'sentence']]\n","\n","# train model\n","model = Word2Vec(sentences, min_count=1)"]},{"cell_type":"markdown","metadata":{"id":"nBN6N-VIu9IR"},"source":["There are many parameters on this constructor for **Word2Vec()**; a few noteworthy arguments you may wish to configure are:\n","* **size**: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n","* **window**: (default 5) The maximum distance between a target word and words around the target word.\n","* **min_count**: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n","* **workers**: (default 3) The number of threads to use while training.\n","* **sg**: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"H0tP8fNGu9IS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685144926074,"user_tz":-330,"elapsed":156,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}},"outputId":"9ff1b5f5-6688-4e95-b1a3-5ea6cb1a9da4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word2Vec<vocab=14, vector_size=100, alpha=0.025>\n"]}],"source":["# summarize the loaded model\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"d1ZENYmru9Ic"},"source":["After the model is trained, it is accessible via the “wv” attribute. This is the actual word vector model in which queries can be made"]},{"cell_type":"code","source":["model.wv.key_to_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7H3YoCXPeK64","executionInfo":{"status":"ok","timestamp":1685145100534,"user_tz":-330,"elapsed":162,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}},"outputId":"ff2eae51-9c1f-435f-c5c9-dcb75c067708"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sentence': 0,\n"," 'the': 1,\n"," 'is': 2,\n"," 'this': 3,\n"," 'final': 4,\n"," 'and': 5,\n"," 'more': 6,\n"," 'one': 7,\n"," 'another': 8,\n"," 'yet': 9,\n"," 'second': 10,\n"," 'word2vec': 11,\n"," 'for': 12,\n"," 'first': 13}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"JZN11YyPu9Ie","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685145114237,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}},"outputId":"3d298525-4c9f-4794-90e3-20b34e81acac"},"outputs":[{"output_type":"stream","name":"stdout","text":["['sentence', 'the', 'is', 'this', 'final', 'and', 'more', 'one', 'another', 'yet', 'second', 'word2vec', 'for', 'first']\n"]}],"source":["# summarize vocabulary\n","words = list(model.wv.key_to_index)\n","# words = list(model.vocab)\n","print(words)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"uqhQf_gvu9Io","outputId":"256f9ce1-b36e-4d7c-fa2e-09dce66dc7e5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685145151241,"user_tz":-330,"elapsed":146,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n","  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n"," -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n","  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n"," -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n","  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n","  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n","  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n","  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n"," -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n"," -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n"," -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n"," -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n"," -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n","  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n"," -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n"," -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n"," -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n","  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n"," -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n"," -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n","  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n"," -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n","  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n","  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n"]}],"source":["# access vector for one word\n","print(model.wv['another'])"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"AMG46lfOu9Ix","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685145166460,"user_tz":-330,"elapsed":166,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}},"outputId":"5fd45a39-2a1f-4e07-9d44-d922dd78ba24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word2Vec<vocab=14, vector_size=100, alpha=0.025>\n"]}],"source":["# save model\n","model.save('model.bin')\n","# load model\n","new_model = Word2Vec.load('model.bin')\n","print(new_model)"]},{"cell_type":"markdown","metadata":{"id":"5X4MqHCTu9Js"},"source":["## Load Google’s Word2Vec Embedding\n","Training your own word vectors may be the best approach for a given NLP problem. But it can take a long time, a fast computer with a lot of RAM and disk space, and perhaps some expertise in finessing the input data and training algorithm.\n","\n","An alternative is to simply use an existing pre-trained word embedding. Along with the paper and code for word2vec, Google also published a pre-trained word2vec model on the <a href='https://code.google.com/archive/p/word2vec/'>Word2Vec Google Code Project</a>\n","\n","A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabytes file. You can download it from here: <a href='https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing'>GoogleNews-vectors-negative300.bin.gz</a>.  Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes.\n","\n","The Gensim library provides tools to load this file. Specifically, you can call the **KeyedVectors.load_word2vec_format()** function to load this model into memory, for example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rhvh5SuvH8r","colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"status":"error","timestamp":1681661329542,"user_tz":-330,"elapsed":40137,"user":{"displayName":"Mayank Parikh","userId":"11709082426090810996"}},"outputId":"d273e269-3ffc-4f65-a4b4-58cae375f225"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-48b0209eeff1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ln -s /content/gdrive/My\\\\ Drive/ /mydrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   )\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    102\u001b[0m     ):\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2m0lERJvYMx"},"outputs":[],"source":["#!ls /mydrive/NITW-NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7f24fJmzW4od"},"outputs":[],"source":["%cd /content\n","ls -l\n","# !wget https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n","#######!unzip /mydrive/NITW-NLP/GoogleNews-vectors-negative300.bin.gz\n","# !gunzip /mydrive/NITW-NLP/GoogleNews-vectors-negative300.bin.gz\n","# ! cp /content/GoogleNews-vectors-negative300.bin  /mydrive/NITW-NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvk0vbIAu9Jt"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","filename = '/mydrive/NITW-NLP/GoogleNews-vectors-negative300.bin'\n","model = KeyedVectors.load_word2vec_format(filename, binary=True)\n","# It takes about a minute to load this"]},{"cell_type":"markdown","metadata":{"id":"Qr71wEKmu9J1"},"source":["An interesting thing that you can do is do a little linear algebra arithmetic with words. For example, a popular example described in lectures and introduction papers is: queen = (king - man) + woman\n","\n","That is the word queen is the closest word given the subtraction of the notion of man from king and adding the word woman. The “man-ness” in king is replaced with “woman-ness” to give us queen. A very cool concept.\n","\n","Gensim provides an interface for performing these types of operations in the **most_similar()** function on the trained or loaded model. For example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvrqJi6RaEXh"},"outputs":[],"source":["model['queen']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2Jor1oZu9J2"},"outputs":[],"source":["# calculate: (king - man) + woman = ?\n","result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"f660U1_bGGl4"},"source":["## Load Stanford’s GloVe Embedding\n","Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short.Generally, NLP practitioners seem to prefer GloVe over Word2Vec at the moment based on results.\n","\n","Like word2vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from.\n","\n","You can download the GloVe pre-trained word vectors and load them easily with gensim.\n","\n","The first step is to convert the GloVe file format to the word2vec file format. The only difference is the addition of a small header line. This can be done by calling the glove2word2vec() function. For example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-I98CJR1G7Gj"},"outputs":[],"source":["# !wget http://nlp.stanford.edu/data/glove.6B.zip\n","# !cp glove.6B.zip /mydrive/NITW-NLP/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDrlONj_KELs"},"outputs":[],"source":["!ls -l /mydrive/NITW-NLP/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8wkP_5xH6nr"},"outputs":[],"source":["%cd /content\n","!unzip /mydrive/NITW-NLP/glove.6B.zip\n","#!gunzip /mydrive/NITW-NLP/GoogleNews-vectors-negative300.bin.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Mva-QWRLFjk"},"outputs":[],"source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","glove_input_file = 'glove.6B.100d.txt'\n","word2vec_output_file = 'word2vec.txt'\n","glove2word2vec(glove_input_file, word2vec_output_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlqpnO_wu9J9"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","# load the Stanford GloVe model\n","filename = 'word2vec.txt'\n","model = KeyedVectors.load_word2vec_format(filename, binary=False)\n","# calculate: (king - man) + woman = ?\n","result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGuHuqGCgiYa"},"outputs":[],"source":["result = model.most_similar(positive=['sad', 'fun'], negative=['negative'], topn=1)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gsBRpgyk6IM"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9ycS-5v_k_78"},"source":["## A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VjeIxNVlJ2T"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ix37fkgElJrr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0605AZwlJf1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFyWEKLKlJVD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0G6QHOxGlJHm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"q-fsdvnTu9I5"},"source":["## Visualize Word Embedding\n","After you learn word embedding for your text data, it can be nice to explore it with visualization. You can use classical projection methods to reduce the high-dimensional word vectors to two-dimensional plots and plot them on a graph. The visualizations can provide a qualitative diagnostic for your learned model.\n","\n","We can retrieve all of the vectors from a trained model as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbgDyIsvu9I7"},"outputs":[],"source":["X = model[model.wv.vocab]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSUOJ37lu9JC"},"outputs":[],"source":["X.shape"]},{"cell_type":"markdown","metadata":{"id":"I-5wfp_nu9JJ"},"source":["We can then train a projection method on the vectors, such as those methods offered in scikit-learn, then use matplotlib to plot the projection as a scatter plot. Let’s look at an example with Principal Component Analysis or PCA.\n","### Plot Word Vectors Using PCA\n","We can create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class as follows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Ov_tTb4u9JL"},"outputs":[],"source":["from gensim.models import Word2Vec\n","from sklearn.decomposition import PCA\n","from matplotlib import pyplot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEO1ReUIu9JS"},"outputs":[],"source":["pca = PCA(n_components=2)\n","result = pca.fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"jGGqlnihu9Jb"},"source":["The resulting projection can be plotted using matplotlib as follows, pulling out the two dimensions as x and y coordinates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_ZrQD-Zu9Jc"},"outputs":[],"source":["pyplot.scatter(result[:, 0], result[:, 1])"]},{"cell_type":"markdown","metadata":{"id":"C_OfpFQtu9Jj"},"source":["We can go one step further and annotate the points on the graph with the words themselves. A crude version without any nice offsets looks as follows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qUYKTQVu9Jk"},"outputs":[],"source":["pyplot.scatter(result[:, 0], result[:, 1])\n","words = list(model.wv.vocab)\n","for i, word in enumerate(words):\n","    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n","pyplot.show()"]},{"cell_type":"markdown","metadata":{"id":"-KqeGfz4u9Jr"},"source":["It is hard to pull much meaning out of the graph given such a tiny corpus was used to fit the model."]}],"metadata":{"colab":{"collapsed_sections":["oGMX97NcEbWI","ldkfQlDaHr8D","33sO1pYiIEAy","5X4MqHCTu9Js","f660U1_bGGl4","q-fsdvnTu9I5","I-5wfp_nu9JJ"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}