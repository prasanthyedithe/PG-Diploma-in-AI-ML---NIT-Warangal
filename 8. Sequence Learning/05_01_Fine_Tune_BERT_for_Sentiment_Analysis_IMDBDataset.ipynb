{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["P0B9IsWOkBNd","YeWGHGZ7s4_y"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## Fine Tune BERT for Sentiment Analysis using IMDBDataset"],"metadata":{"id":"GwPIjm9BXx2n"}},{"cell_type":"markdown","source":["Transformer architecture has encoder and decoder stack, hence called encoder-decoder architecture whereas BERT is just an encoder stack of transformer architecture. There are two variants, BERT-base and BERT-large, which differ in architecture complexity. The base model has 12 layers in the encoder whereas the Large has 24 layers."],"metadata":{"id":"LB7ckMtt7sis"}},{"cell_type":"markdown","source":["BERT was trained on a large text corpus, which gives architecture/model the ability to better understand the language and to learn variability in data patterns and generalizes well on several NLP tasks. As it is bidirectional that means BERT learns information from both the left and the right side of a token’s context during the training phase"],"metadata":{"id":"mC-w9YT578Yc"}},{"cell_type":"markdown","source":["How to Fine-Tune BERT for Text Classification? (https://arxiv.org/pdf/1905.05583.pdf)"],"metadata":{"id":"IcmYfCoSXp0A"}},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"id":"cD5OeEV29M9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Required to save models in HDF5 format\n","\n","# !pip install pyyaml h5py"],"metadata":{"id":"ILZ2LEtapFHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import output\n","# output.enable_custom_widget_manager()"],"metadata":{"id":"MBvni-v666PC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rQdXSTk7gK8"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from transformers import BertTokenizer"]},{"cell_type":"markdown","source":["#### Load IMDB Data"],"metadata":{"id":"OGjimG7J-mRl"}},{"cell_type":"code","source":["(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split = (tfds.Split.TRAIN, tfds.Split.TEST),\n","                                          as_supervised=True, with_info=True)"],"metadata":{"id":"jjvHdlzh8QjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds_info"],"metadata":{"id":"tQUAD0vl5sob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(ds_train), len(ds_test)"],"metadata":{"id":"BIIvDOBZc9A3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Explore IMDB Data"],"metadata":{"id":"UuXab0mO-xef"}},{"cell_type":"code","source":["type(ds_train)"],"metadata":{"id":"Zf1jqgkCWlua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfds.as_numpy(ds_train.take(5))"],"metadata":{"id":"AhWoejZvZRdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for rec in tfds.as_numpy(ds_train.take(5)):\n","  print(rec)"],"metadata":{"id":"4yrqJkNgXCKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for review, label in tfds.as_numpy(ds_train.take(1)):\n","    print(review)\n","    print(type(review))\n","    print(review.decode())\n","    print(type(review.decode()))\n","    print(label)"],"metadata":{"id":"e_7Bo_goZ0x2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = []\n","labels = []\n","for review, label in tfds.as_numpy(ds_train.take(5)):\n","    reviews.append(review.decode())\n","    labels.append(label)\n","    print(review.decode()[0:50], '\\t', label)"],"metadata":{"id":"CBhMe4Fz8uRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews[2]"],"metadata":{"id":"ZIek0qTf8nsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","# classifier1 = pipeline(\"sentiment-analysis\",'bert-base-uncased')\n","classifier = pipeline(\"sentiment-analysis\",'distilbert-base-uncased-finetuned-sst-2-english')"],"metadata":{"id":"0PlSmr5PYMh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier(reviews[1])"],"metadata":{"id":"PikJsYUJYu9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Tokenizer"],"metadata":{"id":"yEefJHmV_K3N"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"i2Als-qA9VPS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let’s prepare the data according to the format needed for the BERT model\n","\n","- Input IDs – The input ids are often the only required parameters to be passed to the model as input. Token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n","\n","- Attention mask – Attention Mask is used to avoid performing attention on padding token indices. Mask value can be either 0 or 1, 1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n","\n","- Token type ids – It is used in use cases like sequence classification or question answering. As these require two different sequences to be encoded in the same input IDs. Special tokens, such as the classifier[CLS] and separator[SEP] tokens are used to separate the sequences."],"metadata":{"id":"p0eWwB0N9hXt"}},{"cell_type":"code","source":["\n","# The encode_plus  function of the tokenizer class will tokenize the raw input, add the special tokens,\n","# and pad the vector to a size equal to max length (that we can set).\n","def convert_example_to_feature(review):\n","  return tokenizer.encode_plus(review,\n","                add_special_tokens = True, # add [CLS], [SEP]\n","                max_length = max_length, # max length of the text that can go to BERT\n","                pad_to_max_length = True, # add [PAD] tokens\n","                return_attention_mask = True, # add attention mask to not focus on pad tokens\n","              )"],"metadata":{"id":"7RoU7Dz19y03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# can be up to 512 for BERT\n","max_length = 512\n","batch_size = 6"],"metadata":{"id":"mhAH9Dw0-Ect"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for review, label in tfds.as_numpy(ds_train.take(1)):\n","    encodedReview = convert_example_to_feature(review.decode())\n","    print(type(encodedReview))\n","    print(encodedReview.keys())\n","    print(encodedReview['input_ids'])\n","    print(encodedReview['token_type_ids'])\n","    print(encodedReview['attention_mask'])\n","    # print(encodedReview)\n","    print(review.decode())\n","    print(label)"],"metadata":{"id":"TJTV7oMNbX1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Encode data"],"metadata":{"id":"uszB0aCPuhMH"}},{"cell_type":"code","source":["# helper functions will help us to transform our raw data to an appropriate format ready to feed into the BERT model\n","def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n","  return {\n","      \"input_ids\": input_ids,\n","      \"token_type_ids\": token_type_ids,\n","      \"attention_mask\": attention_masks,\n","  }, label\n","\n","def encode_examples(ds, limit=-1):\n","  # prepare list, so that we can build up final TensorFlow dataset from slices.\n","  input_ids_list = []\n","  token_type_ids_list = []\n","  attention_mask_list = []\n","  label_list = []\n","  if (limit > 0):\n","      print(\"Using\", limit, \"Records from ds\")\n","      ds = ds.take(limit)\n","  else:\n","      print(\"Using all Records from ds\")\n","  for review, label in tfds.as_numpy(ds):\n","      bert_input = tokenizer.encode_plus(review.decode(),add_special_tokens = True,max_length = max_length,pad_to_max_length = True,\n","                                          return_attention_mask = True,)\n","      # bert_input = convert_example_to_feature(review.decode())\n","      input_ids_list.append(bert_input['input_ids'])\n","      token_type_ids_list.append(bert_input['token_type_ids'])\n","      attention_mask_list.append(bert_input['attention_mask'])\n","      label_list.append([label])\n","  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n"],"metadata":{"id":"gI2z9QcC-JLd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **batch** - Combines consecutive elements of this dataset into batch\n","- **shuffle** - Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements."],"metadata":{"id":"GrmiXST1vDeV"}},{"cell_type":"code","source":["# # train dataset\n","# ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n","# # test dataset\n","# ds_test_encoded = encode_examples(ds_test).batch(batch_size)\n","\n","# train dataset\n","ds_train_encoded = encode_examples(ds_train, limit=5000).shuffle(2000).batch(batch_size)\n","# test dataset\n","ds_test_encoded = encode_examples(ds_test, limit=1000).batch(batch_size)"],"metadata":{"id":"M1SSwGlT-iZs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Understandng ds_train_encoded structure"],"metadata":{"id":"P0B9IsWOkBNd"}},{"cell_type":"code","source":["for record in ds_train_encoded.take(1).as_numpy_iterator():\n","    print(type(record))\n","    # print(len(record))"],"metadata":{"id":"NToTbiC6eMCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n","    print(type(inputs))\n","    print(type(labels))"],"metadata":{"id":"Y1zQyW1Vc2yo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n","    print(inputs.keys())\n","    print(len(labels))"],"metadata":{"id":"ptehrKbheic4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n","    print(len(inputs['input_ids']))\n","    print(len(inputs['token_type_ids']))\n","    print(len(inputs['attention_mask']))\n","    print(len(labels))"],"metadata":{"id":"lFSnzco3eyem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**As we have selected batch_size=6, we see each data record consists of 6 encoded reviews & 6 labels**"],"metadata":{"id":"oBojNCbwfJOq"}},{"cell_type":"code","source":["for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n","    print(inputs['input_ids'][0])\n","    print(inputs['token_type_ids'][0])\n","    print(inputs['attention_mask'][0])\n","    print(labels[0])"],"metadata":{"id":"R5u14j8tfcpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Link google drive"],"metadata":{"id":"pTiwpteHCzyF"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"],"metadata":{"id":"uXzlvvb2kod0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Creating and training (fine tuning) the model"],"metadata":{"id":"pdk6qDqAkIL9"}},{"cell_type":"code","source":["from transformers import TFBertForSequenceClassification\n","import tensorflow as tf\n","# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n","# to avoid catastrophic forgetting (https://arxiv.org/pdf/1905.05583.pdf -- How to Fine-Tune BERT for Text Classification?)\n","learning_rate = 2e-5\n","# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n","number_of_epochs = 1\n","# model initialization\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"],"metadata":{"id":"diH_fqy4-9ra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# choosing Adam optimizer\n","optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate, epsilon=1e-08)\n","# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n","# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"],"metadata":{"id":"eW0aAg0X_FgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["to_train = True"],"metadata":{"id":"s11ihHothj66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The model will take around two hours on GPU to complete training, with just 1 epoch we can achieve over 93% accuracy on validation\n","# you can further increase the epochs and play with other parameters to improve the accuracy.\n","\n","# Training takes about 12 to 15 mins for 5000 training and 1000 testing data rows, and one epoch\n","if to_train:\n","    bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)\n","    # model.save_weights('fineTuneBERTwithIMDB_weights.ckpt')\n","    # !cp fineTuneBERTwithIMDB_weights.* /mydrive"],"metadata":{"id":"bHhPYvRZAuy1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Use previously trained weights (instead of training)"],"metadata":{"id":"VyqvA9ctAaLY"}},{"cell_type":"code","source":["if not to_train:\n","    !cp /mydrive/fineTuneBERT/fineTuneBERTwithIMDB_weights.* /content"],"metadata":{"id":"bKwh6kk6I-1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.load_weights('fineTuneBERTwithIMDB_weights.ckpt')"],"metadata":{"id":"eVX3byrL_a1t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inference on random sample"],"metadata":{"id":"xXmoWdlKA8Z3"}},{"cell_type":"code","source":["# myreview = \"This is a really good movie. I loved it and will watch again\"\n","#myreview = \"There was too much violence and I would suggest to avoid\"\n","myreview = \"Though the movie was good, it was a bit too long\"\n","encoded_myreview = tokenizer.encode(myreview, truncation=True, padding=True, return_tensors=\"tf\")"],"metadata":{"id":"Kj1PateVA-om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_output = model.predict(encoded_myreview)[0]\n","tf_prediction = tf.nn.softmax(tf_output, axis=1)\n","labels = ['Negative','Positive'] #(0:negative, 1:positive)\n","label = tf.argmax(tf_prediction, axis=1)\n","label = label.numpy()\n","print(labels[label[0]])"],"metadata":{"id":"Ru4yFiYcBBuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tf_output)\n","print(tf_prediction)"],"metadata":{"id":"tCVjCXyx_0oE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inference on Test data"],"metadata":{"id":"m0NaVTVXDVmW"}},{"cell_type":"code","source":["reviews = []\n","labels = []\n","for review, label in tfds.as_numpy(ds_test.take(5)):\n","    reviews.append(review.decode())\n","    labels.append(label)\n","    print(review.decode()[0:50], '\\t', label)"],"metadata":{"id":"02xbT_ZyCK2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i = 0\n","reviews[i]"],"metadata":{"id":"_Dgfopf-DI_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# myreview = \"This is a really good movie. I loved it and will watch again\"\n","myreview = reviews[i]\n","encoded_myreview = tokenizer.encode(myreview, truncation=True, padding=True, return_tensors=\"tf\")"],"metadata":{"id":"3R8j3S2nBkpJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_output = model.predict(encoded_myreview)[0]\n","tf_prediction = tf.nn.softmax(tf_output, axis=1)\n","labels = ['Negative','Positive'] #(0:negative, 1:positive)\n","label = tf.argmax(tf_prediction, axis=1)\n","label = label.numpy()\n","print(labels[label[0]])"],"metadata":{"id":"bVxIRCmo2aNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tf_output)\n","print(tf_prediction)"],"metadata":{"id":"yfJgqDDG_8D7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier(myreview)"],"metadata":{"id":"pqAGVL3-D0yX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tokenizer. encode will encode our test example into integers using Bert tokenizer, then we use predict method on the encoded input to get our predictions. The model. predict will return logits, on which we can apply softmax function to get the probabilities for each class, and then using TensorFlow argmax function we can get the class with the highest probability and map it to text labels (positive or negative)."],"metadata":{"id":"FblLPYO-BJLe"}},{"cell_type":"markdown","source":["### Extra code -- to understand how tf.data.Dataset.from_tensor_slices() works"],"metadata":{"id":"3WNLjy8LgTmI"}},{"cell_type":"markdown","source":["#### tf.data.Dataset.from_tensor_slices()\n","https://www.tensorflow.org/api_docs/python/tf/data/Dataset"],"metadata":{"id":"YeWGHGZ7s4_y"}},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n","for element in dataset:\n","  print(element)"],"metadata":{"id":"OlGXwQmMsm2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n","dataset = dataset.map(lambda x: x*2)\n","list(dataset.as_numpy_iterator())"],"metadata":{"id":"oaZlUCiWsqLD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n","for element in dataset.as_numpy_iterator():\n","  print(element)"],"metadata":{"id":"vXJNaLTftt5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n","print(list(dataset.as_numpy_iterator()))"],"metadata":{"id":"vwHqzTE0tv2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n","                                              'b': [5, 6]})\n","list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n","                                      {'a': (2, 4), 'b': 6}]"],"metadata":{"id":"ji-JOIbst0sx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(dataset.as_numpy_iterator())"],"metadata":{"id":"x0chwR7AuF5v"},"execution_count":null,"outputs":[]}]}